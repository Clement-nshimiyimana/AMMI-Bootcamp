This is a pytorch implementation of BERT, a paper implemented by Google in 2018.(https://arxiv.org/abs/1810.04805)

The main Objectives of this Paper are:

.To demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.

• To show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.


• BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/
google-research/bert.
